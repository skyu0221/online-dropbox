{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora 相似问题检测\n",
    "\n",
    "Quora 作为一个高质量的知识平台，需要确保相同的问题不会多次出现。答题者不应该把相同的答案复制粘贴到类似的问题下方，而读者也应该只能在站内找到唯一的问题与他的需求对应。例如，“减体重的最佳方法是什么？”，“如何才能减肥？”，“最有效的减肥计划是什么？”，通常这些问题都会被人们认为是重复提问，因为这些问题的意图都相同。\n",
    "\n",
    "__数据来源__：Data @ Quora https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\n",
    "\n",
    "__目标__：给定任意一句语句，预测一个类似的语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -nc https://raw.githubusercontent.com/skyu0221/online-dropbox/master/ml/capstone2/quora_duplicate_questions.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取并观察数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"quora_duplicate_questions.tsv\", sep ='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近新发现了一个 package 用来检视数据：pandas-profiling，有兴趣可以自己研究一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们只考虑重复问题\n",
    "data = data[data['is_duplicate'] == 1]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 包含多少 data point\n",
    "num_samples = 10000\n",
    "# 最小出现次数数\n",
    "min_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = data['question1'][:num_samples].to_list()\n",
    "q2 = data['question2'][:num_samples].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = []\n",
    "target_words = []\n",
    "for sentence in q1:\n",
    "    input_words.extend(sentence.split())\n",
    "for sentence in q2:\n",
    "    target_words.extend(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = np.array(input_words, dtype=str)\n",
    "unique_input_words, input_count = np.unique(input_words, return_counts=True)\n",
    "target_words = np.array(input_words, dtype=str)\n",
    "unique_target_words, target_count = np.unique(target_words, return_counts=True)\n",
    "len(unique_input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_input_words = unique_input_words[input_count >= min_count]\n",
    "unique_target_words = unique_target_words[target_count >= min_count]\n",
    "len(unique_input_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让程序知道哪里是句子开头，哪里是句子结尾，我们在 target 开头加一个 @，结尾加一个 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = [q.split() for q in q1]\n",
    "q2 = [['@'] + q.split() + ['#'] for q in q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_target_words = np.hstack((unique_target_words, np.array(['@', '#'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in q1:\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        if sentence[i] not in unique_input_words:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "for sentence in q2:\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        if sentence[i] not in unique_target_words:\n",
    "            sentence.pop(i)\n",
    "        else:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q1[0])\n",
    "print(q2[0])\n",
    "print(q1[5])\n",
    "print(q2[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_words\n",
    "del target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(unique_input_words)\n",
    "num_decoder_tokens = len(unique_target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_len = max([len(q) for q in q1])\n",
    "max_decoder_seq_len = max([len(q) for q in q2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Number of samples: ', len(q1))\n",
    "print('Number of unique input tokens (words): ', num_encoder_tokens)\n",
    "print('Number of unique output tokens (words): ', num_decoder_tokens)\n",
    "print('Max seq length for inputs: ', max_encoder_seq_len)\n",
    "print('Max seq length for outputs: ', max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给每一个 token 加一个编号，并创建 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_idx = dict([(token, i) for i, token in enumerate(unique_input_words)])\n",
    "target_token_idx = dict([(token, i) for i, token in enumerate(unique_target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.zeros((len(q1),\n",
    "                          max_encoder_seq_len,\n",
    "                          num_encoder_tokens), dtype = 'float32')\n",
    "decoder_input = np.zeros((len(q1),\n",
    "                          max_decoder_seq_len,\n",
    "                          num_decoder_tokens), dtype = 'float32')\n",
    "decoder_target = np.zeros((len(q1),\n",
    "                           max_decoder_seq_len,\n",
    "                           num_decoder_tokens), dtype = 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder_input 里装的东西和 decoder_target 一样，但是 target 会错开一个 element\n",
    "\n",
    "- 例子 **\"How can I see all my Youtube comments?\"**\n",
    "- decoder_input is **\"How / can / I / see / all / my / Youtube / comments?\"**\n",
    "- decoder_target is **\"can / I / see / all / my / Youtube / comments?**\n",
    "- seq2seq model sees **Input** and predicts **Target**\n",
    "\n",
    "| Input |    |Target    |\n",
    "|---------|-------|-----|\n",
    "|How  | ========>|can  |\n",
    "|can    | ========>      | I    |\n",
    "|I    | ========>      | see    |\n",
    "|see    | ========>      | all   |\n",
    "|all    | ========>      | my    |\n",
    "|my    | ========>      | Youtube    |\n",
    "|Youtube    | ========>      | comments?    |\n",
    "|comments?    | ========>      |     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(zip(q1, q2)):\n",
    "    for t, token in enumerate(x):\n",
    "        encoder_input[i, t, input_token_idx[token]] = 1.\n",
    "    for t, token in enumerate(y):\n",
    "        decoder_input[i, t, target_token_idx[token]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target[i, t-1, target_token_idx[token]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建模型  (seq2seq model)\n",
    "\n",
    "创建两个LSTM 模型 （encoder 和 decoder）\n",
    "<img src=\"https://blog.keras.io/img/seq2seq/seq2seq-inference.png\" style=\"width: 500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "encoder = LSTM(300, return_state = True)\n",
    "_, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape = (None, num_decoder_tokens))\n",
    "lstm = LSTM(300, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _, _ = lstm(decoder_inputs, initial_state = encoder_states)\n",
    "dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
    "decoder_outputs = dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([encoder_input, decoder_input],\n",
    "                    decoder_target,\n",
    "                    batch_size=100,\n",
    "                    epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "plt.plot(loss, label='Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分解模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape = (300, ))\n",
    "decoder_state_input_c = Input(shape = (300, ))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, state_h, state_c = lstm(decoder_inputs,\n",
    "                                         initial_state = decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs,\n",
    "                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_token_idx = dict((i, token) for token, i in input_token_idx.items())\n",
    "reverse_target_token_idx = dict((i, token) for token, i in target_token_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_idx['@']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_token_idx[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '#' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(100):\n",
    "    input_seq = encoder_input[idx: idx+1]\n",
    "    decoded_sent = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', q1[idx])\n",
    "    print('Decoded sentence:', decoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python36464bita83fe77e5d9b4f27b82cb1e4279e3502"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
